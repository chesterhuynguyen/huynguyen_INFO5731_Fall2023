{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "## The third In-class-exercise (due on 11:59 PM 10/08/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2htC-oV70ne"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "Task:\n",
        "Sentiment analysis of customer reviews for a product or service. Sentiment analysis aims to determine whether a given text expresses a positive, negative, or neutral sentiment.\n",
        "This task is valuable for businesses to understand customer opinions, identify areas for improvement, and make data-driven decisions\n",
        "\n",
        "5 features that can be useful for building a machine learning model for sentiment analysis:\n",
        "\n",
        "- Word Frequency: Counting the frequency of specific words or phrases in the text can be informative. Positive or negative sentiment often correlates with certain keywords. For example, words like \"excellent,\" \"amazing,\" or \"terrible\" are strong indicators of sentiment.\n",
        "\n",
        "- N-grams: N-grams are sequences of adjacent words in a text. Analyzing bi-grams (two-word combinations) or tri-grams (three-word combinations) can capture context and sentiment nuances. For instance, \"not good\" is different from \"very good.\"\n",
        "\n",
        "- Sentiment Lexicons: Sentiment lexicons or dictionaries contain lists of words with associated sentiment scores (positive, negative, neutral). By matching words in the text to this lexicon, you can calculate an overall sentiment score for the document. Lexicons can help handle sarcasm or negations, where the sentiment may be reversed.\n",
        "\n",
        "- Part-of-Speech (POS) Tags: Understanding the grammatical structure of the text can be beneficial. For example, identifying adjectives and adverbs in a sentence can provide insights into the intensity of sentiment. Adjectives like \"great\" or adverbs like \"extremely\" can impact the sentiment score.\n",
        "\n",
        "- Emoticons and Emoji: Emoticons and emojis are increasingly used to convey sentiment in text. Detecting and analyzing these symbols can provide valuable information about sentiment, especially in social media data. For example, üòä might indicate a positive sentiment, while üò° suggests a negative sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Sample text data\n",
        "sample_texts = [\n",
        "    \"The movie was excellent. I really enjoyed it!\",\n",
        "    \"The weather today is terrible. I hate the rain.\",\n",
        "    \"I had a great experience at the restaurant. The food was amazing!\",\n",
        "    \"The customer service was awful. I will never go back there.\",\n",
        "    \"I'm so excited for the upcoming vacation! üå¥üòä\"\n",
        "]\n",
        "\n",
        "# Loop through each sample text\n",
        "for idx, text in enumerate(sample_texts, start=1):\n",
        "    print(f\"Sample {idx}:\")\n",
        "    print(\"Text:\", text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # 1. Word Frequency\n",
        "    word_frequency = nltk.FreqDist(words)\n",
        "    print(\"\\nWord Frequency:\")\n",
        "    print(word_frequency)\n",
        "\n",
        "    # 2. N-grams (bi-grams and tri-grams)\n",
        "    bi_grams = list(ngrams(words, 2))\n",
        "    tri_grams = list(ngrams(words, 3))\n",
        "    print(\"\\nBi-grams:\")\n",
        "    print(bi_grams)\n",
        "    print(\"\\nTri-grams:\")\n",
        "    print(tri_grams)\n",
        "\n",
        "    # 3. Sentiment Lexicons (using TextBlob)\n",
        "    tb = TextBlob(text)\n",
        "    sentiment_lexicon_score = tb.sentiment.polarity\n",
        "    print(\"\\nSentiment Lexicon Score:\", sentiment_lexicon_score)\n",
        "\n",
        "    # 4. Part-of-Speech (POS) Tags\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    print(\"\\nPOS Tags:\")\n",
        "    print(pos_tags)\n",
        "\n",
        "    # 5. Emoticons and Emoji (using TextBlob)\n",
        "    emojis = [char for char in text if char in \"üòäüò°\"]\n",
        "    print(\"\\nEmoticons/Emojis:\")\n",
        "    print(emojis)\n",
        "\n",
        "    print(\"\\n-------------------\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAxLTQxm3VQH",
        "outputId": "d8ae375a-0c6b-4426-a4e9-ebf3d7ff607b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1:\n",
            "Text: The movie was excellent. I really enjoyed it!\n",
            "\n",
            "Word Frequency:\n",
            "<FreqDist with 10 samples and 10 outcomes>\n",
            "\n",
            "Bi-grams:\n",
            "[('The', 'movie'), ('movie', 'was'), ('was', 'excellent'), ('excellent', '.'), ('.', 'I'), ('I', 'really'), ('really', 'enjoyed'), ('enjoyed', 'it'), ('it', '!')]\n",
            "\n",
            "Tri-grams:\n",
            "[('The', 'movie', 'was'), ('movie', 'was', 'excellent'), ('was', 'excellent', '.'), ('excellent', '.', 'I'), ('.', 'I', 'really'), ('I', 'really', 'enjoyed'), ('really', 'enjoyed', 'it'), ('enjoyed', 'it', '!')]\n",
            "\n",
            "Sentiment Lexicon Score: 0.8125\n",
            "\n",
            "POS Tags:\n",
            "[('The', 'DT'), ('movie', 'NN'), ('was', 'VBD'), ('excellent', 'JJ'), ('.', '.'), ('I', 'PRP'), ('really', 'RB'), ('enjoyed', 'VBD'), ('it', 'PRP'), ('!', '.')]\n",
            "\n",
            "Emoticons/Emojis:\n",
            "[]\n",
            "\n",
            "-------------------\n",
            "\n",
            "Sample 2:\n",
            "Text: The weather today is terrible. I hate the rain.\n",
            "\n",
            "Word Frequency:\n",
            "<FreqDist with 10 samples and 11 outcomes>\n",
            "\n",
            "Bi-grams:\n",
            "[('The', 'weather'), ('weather', 'today'), ('today', 'is'), ('is', 'terrible'), ('terrible', '.'), ('.', 'I'), ('I', 'hate'), ('hate', 'the'), ('the', 'rain'), ('rain', '.')]\n",
            "\n",
            "Tri-grams:\n",
            "[('The', 'weather', 'today'), ('weather', 'today', 'is'), ('today', 'is', 'terrible'), ('is', 'terrible', '.'), ('terrible', '.', 'I'), ('.', 'I', 'hate'), ('I', 'hate', 'the'), ('hate', 'the', 'rain'), ('the', 'rain', '.')]\n",
            "\n",
            "Sentiment Lexicon Score: -0.9\n",
            "\n",
            "POS Tags:\n",
            "[('The', 'DT'), ('weather', 'NN'), ('today', 'NN'), ('is', 'VBZ'), ('terrible', 'JJ'), ('.', '.'), ('I', 'PRP'), ('hate', 'VBP'), ('the', 'DT'), ('rain', 'NN'), ('.', '.')]\n",
            "\n",
            "Emoticons/Emojis:\n",
            "[]\n",
            "\n",
            "-------------------\n",
            "\n",
            "Sample 3:\n",
            "Text: I had a great experience at the restaurant. The food was amazing!\n",
            "\n",
            "Word Frequency:\n",
            "<FreqDist with 14 samples and 14 outcomes>\n",
            "\n",
            "Bi-grams:\n",
            "[('I', 'had'), ('had', 'a'), ('a', 'great'), ('great', 'experience'), ('experience', 'at'), ('at', 'the'), ('the', 'restaurant'), ('restaurant', '.'), ('.', 'The'), ('The', 'food'), ('food', 'was'), ('was', 'amazing'), ('amazing', '!')]\n",
            "\n",
            "Tri-grams:\n",
            "[('I', 'had', 'a'), ('had', 'a', 'great'), ('a', 'great', 'experience'), ('great', 'experience', 'at'), ('experience', 'at', 'the'), ('at', 'the', 'restaurant'), ('the', 'restaurant', '.'), ('restaurant', '.', 'The'), ('.', 'The', 'food'), ('The', 'food', 'was'), ('food', 'was', 'amazing'), ('was', 'amazing', '!')]\n",
            "\n",
            "Sentiment Lexicon Score: 0.7750000000000001\n",
            "\n",
            "POS Tags:\n",
            "[('I', 'PRP'), ('had', 'VBD'), ('a', 'DT'), ('great', 'JJ'), ('experience', 'NN'), ('at', 'IN'), ('the', 'DT'), ('restaurant', 'NN'), ('.', '.'), ('The', 'DT'), ('food', 'NN'), ('was', 'VBD'), ('amazing', 'VBG'), ('!', '.')]\n",
            "\n",
            "Emoticons/Emojis:\n",
            "[]\n",
            "\n",
            "-------------------\n",
            "\n",
            "Sample 4:\n",
            "Text: The customer service was awful. I will never go back there.\n",
            "\n",
            "Word Frequency:\n",
            "<FreqDist with 12 samples and 13 outcomes>\n",
            "\n",
            "Bi-grams:\n",
            "[('The', 'customer'), ('customer', 'service'), ('service', 'was'), ('was', 'awful'), ('awful', '.'), ('.', 'I'), ('I', 'will'), ('will', 'never'), ('never', 'go'), ('go', 'back'), ('back', 'there'), ('there', '.')]\n",
            "\n",
            "Tri-grams:\n",
            "[('The', 'customer', 'service'), ('customer', 'service', 'was'), ('service', 'was', 'awful'), ('was', 'awful', '.'), ('awful', '.', 'I'), ('.', 'I', 'will'), ('I', 'will', 'never'), ('will', 'never', 'go'), ('never', 'go', 'back'), ('go', 'back', 'there'), ('back', 'there', '.')]\n",
            "\n",
            "Sentiment Lexicon Score: -0.5\n",
            "\n",
            "POS Tags:\n",
            "[('The', 'DT'), ('customer', 'NN'), ('service', 'NN'), ('was', 'VBD'), ('awful', 'JJ'), ('.', '.'), ('I', 'PRP'), ('will', 'MD'), ('never', 'RB'), ('go', 'VB'), ('back', 'RB'), ('there', 'RB'), ('.', '.')]\n",
            "\n",
            "Emoticons/Emojis:\n",
            "[]\n",
            "\n",
            "-------------------\n",
            "\n",
            "Sample 5:\n",
            "Text: I'm so excited for the upcoming vacation! üå¥üòÉ\n",
            "\n",
            "Word Frequency:\n",
            "<FreqDist with 10 samples and 10 outcomes>\n",
            "\n",
            "Bi-grams:\n",
            "[('I', \"'m\"), (\"'m\", 'so'), ('so', 'excited'), ('excited', 'for'), ('for', 'the'), ('the', 'upcoming'), ('upcoming', 'vacation'), ('vacation', '!'), ('!', 'üå¥üòÉ')]\n",
            "\n",
            "Tri-grams:\n",
            "[('I', \"'m\", 'so'), (\"'m\", 'so', 'excited'), ('so', 'excited', 'for'), ('excited', 'for', 'the'), ('for', 'the', 'upcoming'), ('the', 'upcoming', 'vacation'), ('upcoming', 'vacation', '!'), ('vacation', '!', 'üå¥üòÉ')]\n",
            "\n",
            "Sentiment Lexicon Score: 0.46875\n",
            "\n",
            "POS Tags:\n",
            "[('I', 'PRP'), (\"'m\", 'VBP'), ('so', 'RB'), ('excited', 'VBN'), ('for', 'IN'), ('the', 'DT'), ('upcoming', 'JJ'), ('vacation', 'NN'), ('!', '.'), ('üå¥üòÉ', 'NN')]\n",
            "\n",
            "Emoticons/Emojis:\n",
            "['üòÉ']\n",
            "\n",
            "-------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "sample_texts = [\n",
        "    \"The movie was excellent. I really enjoyed it!\",\n",
        "    \"The weather today is terrible. I hate the rain.\",\n",
        "    \"I had a great experience at the restaurant. The food was amazing!\",\n",
        "    \"The customer service was awful. I will never go back there.\",\n",
        "    \"I'm so excited for the upcoming vacation! üå¥üòä\"\n",
        "]\n",
        "\n",
        "labels = [1, 0, 1, 0, 1]\n",
        "\n",
        "# Feature extraction\n",
        "features = []\n",
        "\n",
        "for text in sample_texts:\n",
        "    words = nltk.word_tokenize(text)\n",
        "    bi_grams = list(ngrams(words, 2))\n",
        "    tri_grams = list(ngrams(words, 3))\n",
        "    tb = TextBlob(text)\n",
        "    sentiment_lexicon_score = tb.sentiment.polarity\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    emojis = [char for char in text if char in \"üòäüò°\"]\n",
        "\n",
        "    # Combine all features into a single feature vector\n",
        "    feature_vector = [\n",
        "        len(words),  # Word Frequency\n",
        "        len(bi_grams) + len(tri_grams),  # N-grams\n",
        "        sentiment_lexicon_score,  # Sentiment Lexicons\n",
        "        len([tag for word, tag in pos_tags]),  # Part-of-Speech Tags\n",
        "        len(emojis)  # Emoticons/Emojis\n",
        "    ]\n",
        "\n",
        "    features.append(feature_vector)\n",
        "\n",
        "# Train a random forest classifier to rank the features\n",
        "classifier = RandomForestClassifier(random_state=42)\n",
        "classifier.fit(features, labels)\n",
        "\n",
        "# Get feature importance scores from the classifier\n",
        "feature_importances = classifier.feature_importances_\n",
        "\n",
        "# Create a list of feature names\n",
        "feature_names = [\n",
        "    'Word Frequency',\n",
        "    'N-grams',\n",
        "    'Sentiment Lexicons',\n",
        "    'Part-of-Speech Tags',\n",
        "    'Emoticons/Emojis'\n",
        "]\n",
        "\n",
        "# Combine feature names and their importance scores\n",
        "feature_ranking = list(zip(feature_names, feature_importances))\n",
        "\n",
        "# Sort the features by their importance scores in descending order\n",
        "feature_ranking.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the top 5 ranked features\n",
        "print(\"Top 5 Ranked Features:\")\n",
        "for feature, importance in feature_ranking[:5]:\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD3bZTPQ6-dS",
        "outputId": "3c218441-ee4c-470e-f2b1-bb43c44c5927"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Ranked Features:\n",
            "Sentiment Lexicons: 0.3753\n",
            "Part-of-Speech Tags: 0.2448\n",
            "Word Frequency: 0.1792\n",
            "N-grams: 0.1738\n",
            "Emoticons/Emojis: 0.0269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z2vUbAoDQGo",
        "outputId": "7606f4b4-324a-490b-827c-9dd7559dd7c6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample text data\n",
        "sample_texts = [\n",
        "    \"The movie was excellent. I really enjoyed it!\",\n",
        "    \"The weather today is terrible. I hate the rain.\",\n",
        "    \"I had a great experience at the restaurant. The food was amazing!\",\n",
        "    \"The customer service was awful. I will never go back there.\",\n",
        "    \"I'm so excited for the upcoming vacation! üå¥üòÉ\"\n",
        "]\n",
        "\n",
        "# Define your query\n",
        "query = \"I loved the movie; it was fantastic!\"\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the query and text data\n",
        "query_tokens = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "text_tokens = tokenizer(sample_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)  # Adjust max_length as needed\n",
        "\n",
        "# Encode the query and text data with BERT\n",
        "query_output = model(**query_tokens)\n",
        "text_output = model(**text_tokens)\n",
        "\n",
        "# Calculate cosine similarity between the query and each text\n",
        "query_embedding = query_output.last_hidden_state.mean(dim=1).detach().numpy()  # Use mean pooling for the query\n",
        "text_embeddings = text_output.last_hidden_state.mean(dim=1).detach().numpy()  # Use mean pooling for the text data\n",
        "\n",
        "similarities = cosine_similarity(query_embedding, text_embeddings)\n",
        "\n",
        "# Rank the text data based on similarity scores in descending order\n",
        "ranked_texts = sorted(enumerate(sample_texts), key=lambda x: similarities[0][x[0]], reverse=True)\n",
        "\n",
        "# Print the ranked text data and their similarity scores\n",
        "print(\"Ranked Texts Based on Similarity:\")\n",
        "for idx, (similarity, text) in enumerate(ranked_texts):\n",
        "    print(f\"Rank {idx + 1}: Similarity Score = {similarity:.4f}\")\n",
        "    print(text)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_S0H3E6vIj9_",
        "outputId": "7c456f4f-d704-4eca-87ea-573856b531a9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Texts Based on Similarity:\n",
            "Rank 1: Similarity Score = 0.0000\n",
            "The movie was excellent. I really enjoyed it!\n",
            "\n",
            "Rank 2: Similarity Score = 2.0000\n",
            "I had a great experience at the restaurant. The food was amazing!\n",
            "\n",
            "Rank 3: Similarity Score = 4.0000\n",
            "I'm so excited for the upcoming vacation! üå¥üòÉ\n",
            "\n",
            "Rank 4: Similarity Score = 3.0000\n",
            "The customer service was awful. I will never go back there.\n",
            "\n",
            "Rank 5: Similarity Score = 1.0000\n",
            "The weather today is terrible. I hate the rain.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}