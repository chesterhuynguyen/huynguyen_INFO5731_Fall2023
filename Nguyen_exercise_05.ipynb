{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoL92aYxXrQG"
      },
      "source": [
        "# **The fifth in-class-exercise (40 points in total, 4/18/2023)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXAiSHiKXrQI"
      },
      "source": [
        "(20 points) The purpose of the question is to practice different machine learning algorithms for text classification as well as the performance evaluation. In addition, you are requried to conduct *10 fold cross validation (https://scikit-learn.org/stable/modules/cross_validation.html)* in the training.\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n",
        "\n",
        "Algorithms:\n",
        "\n",
        "(1) MultinominalNB\n",
        "\n",
        "(2) SVM\n",
        "\n",
        "(3) KNN\n",
        "\n",
        "(4) Decision tree\n",
        "\n",
        "(5) Random Forest\n",
        "\n",
        "(6) XGBoost\n",
        "\n",
        "(7) Word2Vec\n",
        "\n",
        "(8) BERT\n",
        "\n",
        "Evaluation measurement:\n",
        "\n",
        "(1) Accuracy\n",
        "\n",
        "(2) Recall\n",
        "\n",
        "(3) Precison\n",
        "\n",
        "(4) F-1 score"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O3nqN2V7rOld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopword = nltk.corpus.stopwords.words('english')\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "# Define the process function\n",
        "def process(review):\n",
        "    review = \"\".join([word.lower() for word in review if word not in string.punctuation])\n",
        "    review = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", review)\n",
        "    tokens = re.split('\\W+', review)\n",
        "    review = [wl.lemmatize(word) for word in tokens if word not in stopword]\n",
        "    return review\n",
        "\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "tfidf_vect = TfidfVectorizer(analyzer=process)\n",
        "\n",
        "# Fit and transform training & test data\n",
        "X_tfidf = tfidf_vect.fit_transform(train_data['reviews'])\n",
        "X_test_tfidf = tfidf_vect.transform(test_data['reviews'])\n",
        "X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
        "X_tfidf_df.columns = tfidf_vect.get_feature_names_out()\n",
        "\n",
        "\n",
        "# Sample the training set\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_tfidf_df, train_data['sentiment'].values,\n",
        "                                                    test_size=0.2, random_state=42)\n",
        "\n",
        "# 1) MultinomialNB\n",
        "mnb = MultinomialNB()\n",
        "model_mnb = mnb.fit(x_train, y_train)\n",
        "y_pred_mnb = model_mnb.predict(x_test)\n",
        "print('MultinomialNB Results:')\n",
        "print('Accuracy: %s' % accuracy_score(y_pred_mnb, y_test))\n",
        "print(classification_report(y_test, y_pred_mnb))\n",
        "\n",
        "# 2) LinearSVC\n",
        "svm = LinearSVC()\n",
        "model_svm = svm.fit(x_train, y_train)\n",
        "y_pred_svm = model_svm.predict(x_test)\n",
        "print('\\nLinearSVC Results:')\n",
        "print('Accuracy: %s' % accuracy_score(y_pred_svm, y_test))\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "\n",
        "# 3) KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
        "model_knn = knn.fit(x_train, y_train)\n",
        "y_pred_knn = model_knn.predict(x_test)\n",
        "print('\\nKNeighborsClassifier Results:')\n",
        "print('Accuracy: %s' % accuracy_score(y_pred_knn, y_test))\n",
        "print(classification_report(y_test, y_pred_knn))\n",
        "\n",
        "# 4) Decision Tree\n",
        "dt = DecisionTreeClassifier()\n",
        "model_dt = dt.fit(x_train, y_train)\n",
        "y_pred_dt = model_dt.predict(x_test)\n",
        "print('\\nDecision Tree Results:')\n",
        "print('Accuracy: %s' % accuracy_score(y_pred_dt, y_test))\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "\n",
        "# 5) Random Forest\n",
        "rf = RandomForestClassifier()\n",
        "model_rf = rf.fit(x_train, y_train)\n",
        "y_pred_rf = model_rf.predict(x_test)\n",
        "print('\\nRandom Forest Results:')\n",
        "print('Accuracy: %s' % accuracy_score(y_pred_rf, y_test))\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# 6) XGBoost\n",
        "xgb = XGBClassifier()\n",
        "\n",
        "# Convert target variable to integers\n",
        "y_train_int = y_train.astype(int)\n",
        "y_test_int = y_test.astype(int)\n",
        "\n",
        "model_xgb = xgb.fit(x_train, y_train_int)\n",
        "ypred_xgb = model_xgb.predict(x_test)\n",
        "print('\\nXGBoost Results:')\n",
        "print('Accuracy: %s' % accuracy_score(ypred_xgb, y_test_int))\n",
        "print(classification_report(y_test_int, ypred_xgb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kycTwVwdy3Co",
        "outputId": "484e5c4a-090b-49dc-987c-9cd2935ffc96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultinomialNB Results:\n",
            "Accuracy: 0.7955202312138728\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.70      0.77       671\n",
            "           1       0.76      0.88      0.82       713\n",
            "\n",
            "    accuracy                           0.80      1384\n",
            "   macro avg       0.80      0.79      0.79      1384\n",
            "weighted avg       0.80      0.80      0.79      1384\n",
            "\n",
            "\n",
            "LinearSVC Results:\n",
            "Accuracy: 0.791907514450867\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.75      0.78       671\n",
            "           1       0.78      0.83      0.80       713\n",
            "\n",
            "    accuracy                           0.79      1384\n",
            "   macro avg       0.79      0.79      0.79      1384\n",
            "weighted avg       0.79      0.79      0.79      1384\n",
            "\n",
            "\n",
            "KNeighborsClassifier Results:\n",
            "Accuracy: 0.740606936416185\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.71      0.73       671\n",
            "           1       0.74      0.77      0.75       713\n",
            "\n",
            "    accuracy                           0.74      1384\n",
            "   macro avg       0.74      0.74      0.74      1384\n",
            "weighted avg       0.74      0.74      0.74      1384\n",
            "\n",
            "\n",
            "Decision Tree Results:\n",
            "Accuracy: 0.6604046242774566\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.61      0.64       671\n",
            "           1       0.66      0.71      0.68       713\n",
            "\n",
            "    accuracy                           0.66      1384\n",
            "   macro avg       0.66      0.66      0.66      1384\n",
            "weighted avg       0.66      0.66      0.66      1384\n",
            "\n",
            "\n",
            "Random Forest Results:\n",
            "Accuracy: 0.7485549132947977\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.68      0.73       671\n",
            "           1       0.73      0.81      0.77       713\n",
            "\n",
            "    accuracy                           0.75      1384\n",
            "   macro avg       0.75      0.75      0.75      1384\n",
            "weighted avg       0.75      0.75      0.75      1384\n",
            "\n",
            "\n",
            "XGBoost Results:\n",
            "Accuracy: 0.7189306358381503\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.60      0.67       671\n",
            "           1       0.69      0.83      0.75       713\n",
            "\n",
            "    accuracy                           0.72      1384\n",
            "   macro avg       0.73      0.72      0.71      1384\n",
            "weighted avg       0.73      0.72      0.71      1384\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXM470CBXrQJ"
      },
      "source": [
        "(20 points) The purpose of the question is to practice different machine learning algorithms for text clustering\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "Apply the listed clustering methods to the dataset:\n",
        "\n",
        "K-means\n",
        "\n",
        "DBSCAN\n",
        "\n",
        "Hierarchical clustering\n",
        "\n",
        "Word2Vec\n",
        "\n",
        "BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"Restaurant_reviews.csv\")\n",
        "\n",
        "# Fill NaN values with an empty string\n",
        "documents = documents.fillna(\"\")\n",
        "documents = data['Review'].astype(str).tolist()\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# K-means clustering\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Evaluate K-means using silhouette score\n",
        "silhouette_avg = silhouette_score(X, kmeans_labels)\n",
        "print(f\"K-means Silhouette Score: {silhouette_avg}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-kPkIAFcgH-",
        "outputId": "1758486c-9855-4463-ce24-da474ff5049c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-means Silhouette Score: 0.9010087115740629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"Restaurant_reviews.csv\")\n",
        "\n",
        "# Fill NaN values with an empty string\n",
        "df['Review'] = df['Review'].fillna(\"\")\n",
        "documents = df['Review'].astype(str).tolist()\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan_labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Evaluate DBSCAN using silhouette score\n",
        "silhouette_avg = silhouette_score(X, dbscan_labels)\n",
        "print(f\"DBSCAN Silhouette Score: {silhouette_avg}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON0hhLoEfhwq",
        "outputId": "bb4ab9d2-d61a-488a-d050-5e747857c55d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DBSCAN Silhouette Score: 0.8726715344587588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"Restaurant_reviews.csv\")\n",
        "\n",
        "# Fill NaN values with an empty string\n",
        "df['Review'] = df['Review'].fillna(\"\")\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(df['Review'].astype(str))\n",
        "\n",
        "# Hierarchical clustering\n",
        "n_clusters = 5\n",
        "agg_cluster = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "agg_labels = agg_cluster.fit_predict(X.toarray())\n",
        "\n",
        "# Evaluate AHierarchical clustering clustering using silhouette score\n",
        "silhouette_avg = silhouette_score(X, agg_labels)\n",
        "print(f\"Hierarchical clustering Silhouette Score: {silhouette_avg}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjUEN0NSgEsH",
        "outputId": "2b78f010-1ad3-4288-836e-d0edc6e0724b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agglomerative Silhouette Score: 0.8738704064724554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"Restaurant_reviews.csv\")\n",
        "\n",
        "# Fill NaN values with an empty string\n",
        "df['Review'] = df['Review'].fillna(\"\")\n",
        "documents = df['Review'].astype(str).tolist()\n",
        "\n",
        "# Tokenize the sentences into words\n",
        "tokenized_documents = [doc.split() for doc in documents]\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=tokenized_documents, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Get word vectors\n",
        "word_vectors = word2vec_model.wv\n",
        "\n",
        "# Create a document vector by averaging word vectors\n",
        "document_vectors = [\n",
        "    sum(word_vectors[word] for word in doc) / len(doc) if len(doc) > 0 else [0] * 100\n",
        "    for doc in tokenized_documents\n",
        "]\n",
        "\n",
        "# Apply K-means clustering to document vectors\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(document_vectors)\n",
        "\n",
        "# Evaluate K-means using silhouette score\n",
        "silhouette_avg = silhouette_score(document_vectors, kmeans_labels)\n",
        "print(f\"K-means Silhouette Score: {silhouette_avg}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpFuj7Ahj4eu",
        "outputId": "05f9efe0-e2b3-4016-ffce-a926ad10505a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-means Silhouette Score: 0.9509464009624111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccazqHxEXrQJ"
      },
      "source": [
        "In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS9Uba6AXrQJ"
      },
      "outputs": [],
      "source": [
        "#You can write you answer here. (No code needed)\n",
        "\n",
        "#The silhouette scores obtained from the clustering methods provide a measure of the quality and separation of clusters within the Restaurant Reviews dataset.\n",
        "#Among the methods evaluated, Word2Vec achieved the highest silhouette score of 0.9509, indicating well-defined and distinct clusters in the embedding space.\n",
        "#K-means follows closely with a score of 0.9010, suggesting strong cluster cohesion and separation.\n",
        "#Hierarchical clustering and DBSCAN exhibit slightly lower silhouette scores of 0.8739 and 0.8727, respectively.\n",
        "#While these values still indicate reasonable clustering performance, the higher score of Word2Vec suggests that leveraging word embeddings might capture more nuanced relationships within the textual data, resulting in more cohesive and well-separated clusters compared to traditional clustering approaches.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}