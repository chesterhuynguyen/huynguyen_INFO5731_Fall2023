{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chesterhuynguyen/huynguyen_INFO5731_Fall2023/blob/main/Huy_Nguyen_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from [Semantic Scholar](https://www.semanticscholar.org).\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 reddits by using a hashtag (you can use any hashtag) from Reddits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2a75ff8-b305-4937-a6da-a5f83c7dfa3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 reviews have been saved to Reviews.csv.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# The URL of \"Black Pather: Wakanda Forever\" movie's IMDB page with user reviews.\n",
        "url = \"https://www.imdb.com/title/tt9114286/reviews\"\n",
        "\n",
        "# Create a function to scrape user reviews from IMDB.\n",
        "def scrape_imdb_reviews(url, num_reviews=1000):\n",
        "    reviews = []\n",
        "    page_num = 1\n",
        "\n",
        "    while len(reviews) < num_reviews:\n",
        "        # Fetch the IMDB page.\n",
        "        page_url = f\"{url}?start={page_num}\"\n",
        "        response = requests.get(page_url)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to retrieve page {page_url}\")\n",
        "            break\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Extract user reviews from the page.\n",
        "        review_elements = soup.find_all(\"div\", class_=\"text show-more__control\")\n",
        "        for review_element in review_elements:\n",
        "            review_text = review_element.get_text(strip=True)\n",
        "            reviews.append(review_text)\n",
        "\n",
        "        # Move to the next page.\n",
        "        page_num += 1\n",
        "\n",
        "    return reviews\n",
        "\n",
        "# Scrape user reviews (up to 1000).\n",
        "user_reviews = scrape_imdb_reviews(url, num_reviews=1000)\n",
        "\n",
        "# Save the user reviews to a CSV file.\n",
        "csv_file = \"Reviews.csv\"\n",
        "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Review\"])\n",
        "    for review in user_reviews:\n",
        "        writer.writerow([review])\n",
        "\n",
        "print(f\"{len(user_reviews)} reviews have been saved to {csv_file}.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas nltk textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I5BPZVyi0aD",
        "outputId": "a5e76974-8310-43dc-aca3-534276f74558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b372530c-7329-48e7-ac47-4222aa32981f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data cleaning and saved in the same CSV file.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import Word\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Load the CSV file containing the user reviews.\n",
        "csv_file = \"Reviews.csv\"\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Define a function for text cleaning.\n",
        "def clean_text(text):\n",
        "    # Remove special characters and punctuation.\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = \" \".join(word for word in text.split() if word.isalnum())\n",
        "\n",
        "    # Remove numbers.\n",
        "    text = \" \".join(word for word in text.split() if not word.isnumeric())\n",
        "\n",
        "    # Remove stopwords.\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    text = \" \".join(word for word in text.split() if word not in stop_words)\n",
        "\n",
        "    # Lowercase all text.\n",
        "    text = text.lower()\n",
        "\n",
        "    # Stemming and Lemmatization.\n",
        "    text = \" \".join(Word(word).lemmatize() for word in text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function to the \"Review\" column.\n",
        "df[\"Cleaned_Review\"] = df[\"Review\"].apply(clean_text)\n",
        "\n",
        "# Save the cleaned data in the same CSV file.\n",
        "df.to_csv(csv_file, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"Data cleaning and saved in the same CSV file.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spacy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Qo98-z043--",
        "outputId": "5dddfb6c-fda6-4f10-b57e-fe328a338c03"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file = \"Reviews.csv\"\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Download the NLTK data for POS tagging.\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "\n",
        "# Define a function for POS tagging and counting.\n",
        "def pos_tag_and_count(text):\n",
        "    # Tokenize the text into words.\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Perform POS tagging.\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "    # Initialize counts for Nouns, Verbs, Adjectives, and Adverbs.\n",
        "    noun_count = 0\n",
        "    verb_count = 0\n",
        "    adj_count = 0\n",
        "    adv_count = 0\n",
        "\n",
        "    # Define POS tag prefixes for Nouns, Verbs, Adjectives, and Adverbs.\n",
        "    noun_prefixes = [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]\n",
        "    verb_prefixes = [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
        "    adj_prefixes = [\"JJ\", \"JJR\", \"JJS\"]\n",
        "    adv_prefixes = [\"RB\", \"RBR\", \"RBS\"]\n",
        "\n",
        "    # Count the POS tags.\n",
        "    for word, pos in pos_tags:\n",
        "        if pos in noun_prefixes:\n",
        "            noun_count += 1\n",
        "        elif pos in verb_prefixes:\n",
        "            verb_count += 1\n",
        "        elif pos in adj_prefixes:\n",
        "            adj_count += 1\n",
        "        elif pos in adv_prefixes:\n",
        "            adv_count += 1\n",
        "\n",
        "    return {\"Noun\": noun_count, \"Verb\": verb_count, \"Adjective\": adj_count, \"Adverb\": adv_count}\n",
        "\n",
        "# Apply POS tagging and counting to the \"Cleaned_Review\" column.\n",
        "df[\"POS_Counts\"] = df[\"Cleaned_Review\"].apply(pos_tag_and_count)\n",
        "\n",
        "# Calculate the total counts for each POS.\n",
        "total_counts = df[\"POS_Counts\"].apply(pd.Series).sum()\n",
        "\n",
        "print(\"Total POS Counts:\")\n",
        "print(total_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AQ7rUQtkS7x",
        "outputId": "70683925-7276-4152-d43a-21b3c7b89e94"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total POS Counts:\n",
            "Noun         59840\n",
            "Verb         30600\n",
            "Adjective    33520\n",
            "Adverb       16760\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load the CSV file containing the clean text data.\n",
        "csv_file = \"Reviews.csv\"\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Load the spaCy English model.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a function to extract and count named entities.\n",
        "def extract_and_count_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entity_counts = {\"Person\": 0, \"Organization\": 0, \"Location\": 0, \"Product\": 0, \"Date\": 0}\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        entity_type = ent.label_\n",
        "        if entity_type in entity_counts:\n",
        "            entity_counts[entity_type] += 1\n",
        "\n",
        "    return entity_counts\n",
        "\n",
        "# Apply entity extraction and counting to the \"Cleaned_Review\" column.\n",
        "df[\"Entity_Counts\"] = df[\"Cleaned_Review\"].apply(extract_and_count_entities)\n",
        "\n",
        "# Calculate the total counts for each entity type.\n",
        "total_counts = df[\"Entity_Counts\"].apply(pd.Series).sum()\n",
        "\n",
        "# Print the entity counts.\n",
        "print(\"Total Entity Counts:\")\n",
        "print(total_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-JrDyaPv0jD",
        "outputId": "45c201ad-40c4-4ae0-dc8c-34943e9d41c1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Entity Counts:\n",
            "PERSON     920\n",
            "ORG        320\n",
            "LOC         40\n",
            "PRODUCT     40\n",
            "DATE       240\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}